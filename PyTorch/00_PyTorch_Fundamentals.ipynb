{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcU0ptXnh8Aw"
      },
      "source": [
        "# Let's start with a brief definition of machine learning just as a reminder\n",
        "\n",
        "> ML is turning any data  into numbers and finding patterns in those numbers\n",
        "\n",
        "This notes are code focus, so the math will be handled behind the scene.\n",
        "\n",
        "- For now just consider the basic paradigm behind ML vs traditional programming: **ML** receives inputs and outputs to figure out rules (over-simplification), while **traditional programming** receive inputs and rules that guide to an output.\n",
        "\n",
        "**You might want to use ML and DL for:**\n",
        "- Problems with lon lists of rules\n",
        "- Continually changing environments\n",
        "- Discovering insights within large sets of data\n",
        "\n",
        "**You might not want to use ML and DL for:**\n",
        "- Need explainability â€” the patterns learned are usually uninterpretable by a human\n",
        "- When traditional programming offers better results\n",
        "- When errors are unacceptable\n",
        "- When there is not a lot of data (unless you have a specific approach to this)\n",
        "\n",
        "**Typically you will:**\n",
        "- Use ML for structured data\n",
        "- Use DP for unstructured data\n",
        "\n",
        "_However, depending on how you represent your problem, you can use either ML or DL algorithms to solve it_\n",
        "\n",
        "**Anatomy of Neural Networks:**\n",
        "* Input layer: data goes here\n",
        "* Hidden Layer(s): learns patterns in data (numerical representations) from a set number of neurons\n",
        ">'Patterns' is an arbitrary term used interchangeably with 'embedding', 'weights', 'feature representation', 'feature vectors', all referring to similar things.\n",
        "* Output layers: output learned representation or prediction probabilities\n",
        "\n",
        "_There are some very complex architectures (Like the computer vision's popular resnet152 which has 152 hidden layers) as there are single hidden layered NN._ Each layer is usually combination of linear (straight line) and/or non-linear (non-straight line) functions.\n",
        "\n",
        "## Types of learning\n",
        "\n",
        "- Supervised: Has data with labels for all\n",
        "- Semi-supervised: Some data with labels used to train a model that would then be used to train the data without labels\n",
        "- Unsupervised: Data with no labels (idk what is in the dataset, so look what you can find)\n",
        "- Transfer: When a model has learned patterns from a set of data, those patterns can be used in another problem for another set of data\n",
        "- Reinforcement learning: using a system of action and reward to feed the NN\n",
        "\n",
        "## What is Deep Learning actually used for\n",
        "\n",
        ">ðŸ’¡ Start looking almost every experience as something that can be translated into numbers and how to actually do that\n",
        "\n",
        "- ****Some use cases:****\n",
        "    - Recommendation\n",
        "    - Translation\n",
        "    - Speech recognition\n",
        "    - Computer vision\n",
        "    - NLP\n",
        "- ****************Deepmind :**************** They converted proteins to numbers and there the patterns were hidden\n",
        "\n",
        "## What is PyTorch\n",
        "> Most popular research deeplearning framework, able to run on GPU/GPUs and access many pre-built deep learning models.\n",
        "- It is whole stack: preprocess data, model data, deploy model in app/cloud\n",
        "- Originally designed and used in-house by Meta(formerly Facebook), now open-source and used by companies such such as Tesla, Microsoft, openAI.\n",
        "\n",
        "## Why Pytorch?\n",
        "> Other than being used by 58% of papers up to dec 2021, this is a tool that allows virtually anyone solve problems that would have taken a whole team and lots of investment back in 2014\n",
        "Some players in AI using pytorch are:\n",
        "- [Tesla](https://youtu.be/oBklltKXtDE)\n",
        "- [OpenAI](https://openai.com/blog/openai-pytorch)\n",
        "- [The incredible PyTorch project](https://github.com/NurmaU/incredible_pytorch)\n",
        "- _AND_ it allows to run code on GPU (using CUDA based interfaces used for general purpose computing that allow tu run numerical computations)\n",
        "\n",
        "## What is a tensor?\n",
        "> You have inputs, representations, and outputs: all numerically encoded. Therefore, a tensor is any kind of n-rank, m-dimensional numerical encoding used to represent data in a way that a human would not understand though a computer would.\n",
        "_However, a tensor is not a fixed concept ([for a broad yet comprehensive explanation check this out](https://www.youtube.com/watch?v=f5liqUk0ZTw))_\n",
        "\n",
        "## So, the roadpath (or so I expect, remember life laughs at previsions):\n",
        "* Dealing with tensors and tensor operations (Just like we did in TensorFlow)\n",
        "* Preprocessing Data (getting it into tensors)\n",
        "* Building and using pretrained DL models\n",
        "* Fitting a model to data (learning patterns)\n",
        "* Making predictions with a model (using models)\n",
        "* Evaluating model predictions\n",
        "* Saving and loading models\n",
        "* Using a trained model to make predictions on custom data\n",
        "### How: we are cooks, not chemists.\n",
        "> A little bit of science, a little bit of art.\n",
        "## A PyTorch Workflow (taken from ZtM)\n",
        "![Screenshot 2023-09-24 at 13.11.52.png](<attachment:Screenshot 2023-09-24 at 13.11.52.png>)\n",
        "### But how do I approach AI?\n",
        "> Learning and coding AI are two different things. Either focus on coding along this course notes, explore and experiment, visualize what you don't understand, ask questions, whatever you choose, justo go for it. But most importantly: _**Do something and share your work**_\n",
        "\n",
        "Please, avoid the \"I can't learn\" mindset, burning yourself out is no good for your brain.\n",
        "\n",
        "## Here is where the code starts so hold on tight, we are done with the nice titles and friendly definitions, lol\n",
        "\n",
        "Btw, the markdown notes are currently being written in vscode while the code is wrote and executed in Colab Notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFLSJV39h8Az",
        "outputId": "03f75508-b015-4dbe-f2fe-0a9569893675"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "# Let's import all necessary libraries for our runtime\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(torch.__version__) # Check the torch version we are working with\n",
        "\n",
        "# We won't be using GPUs just yet. Let's go step by step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVlddbNvi0c5"
      },
      "source": [
        "## Introduction to Tensors\n",
        "\n",
        "### Creating tensors\n",
        "\n",
        "PyTorch tensors are created using `torch.Tensor()` = https://pytorch.org/docs/stable/tensors.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubHuLPlCi8G5",
        "outputId": "8450f529-44e9-4ca8-fac4-6121d397a01e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(7)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# scalar // There are different types of tensors\n",
        "scalar = torch.tensor(7)\n",
        "scalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YPSVb5cjSQS",
        "outputId": "09a3dd28-c6ce-4d9f-c325-2e787edabb36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the dimensions of a tensor\n",
        "scalar.ndim # A 0D tensor is nothing but a scalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1WKqtj6jljo",
        "outputId": "afc146a5-0db0-42ca-90f8-3fbccd496119"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# As a result, a scalar can be reached as a singular Python int\n",
        "scalar.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlMOA4N-j77i",
        "outputId": "5a47449b-83b3-478e-e8dc-530f3d6d15e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([7, 7])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a vector // has a magnitude and a dimension and is >1 number\n",
        "vector = torch.tensor([7,7])\n",
        "vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBjbZIqetUen",
        "outputId": "1071e380-9493-4505-ab34-822f4e628610"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector.ndim # 1D, one way to check it is the number of pairs of closing square brackets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4a_07sutbX2",
        "outputId": "ae32f96d-fc5a-49b0-9c5e-ae0530ea674d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector.shape # It is different to the dimension as it is the numbers of elements inside each dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c28vnZndth9M",
        "outputId": "a026f7a0-cda1-4f29-ef62-b86ebb783b7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 7,  8],\n",
              "        [ 9, 10]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# MATRIX\n",
        "MATRIX = torch.tensor([\n",
        "    [7,8],\n",
        "    [9,10]\n",
        "])\n",
        "MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F8y4RVltpfk",
        "outputId": "5e5337f6-405c-4d12-f013-34d676d6ce26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MATRIX.ndim # Matrix are 2 dimensional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNCLPzrutrVk",
        "outputId": "a736aa56-4c50-43b1-bcd3-83d8ea642ea5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MATRIX.shape # This will output the elements in each dimension from outer to inner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5juM-I9tsXR",
        "outputId": "9637e6be-b0cc-4f74-c6d5-68fb716da2e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1, 2, 3],\n",
              "         [3, 6, 9],\n",
              "         [2, 4, 5]]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TENSOR\n",
        "TENSOR = torch.tensor([[[1,2,3],\n",
        "                        [3,6,9],\n",
        "                        [2,4,5]]]) # Most times you won't do this manually, just understand how this fundamentally works\n",
        "TENSOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evPJa0Mdt573",
        "outputId": "c768b7d8-aef6-4869-a643-204f5270ecc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " TENSOR.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPCaqECQuGNl",
        "outputId": "b5af1963-9e5b-4176-e4d3-2aef968140f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 3])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TENSOR.shape # Again, here the dimension 0 [outtest pair of brackets] has 1 element, dimension 1 has 3 and dimension 2 has 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA5nMypLuMnk",
        "outputId": "20ffb539-64fb-468e-e49b-8a68fea6ef1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[1, 2, 3],\n",
              "         [3, 6, 9],\n",
              "         [2, 4, 5]]),\n",
              " tensor(9))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TENSOR[0], TENSOR[0][1][2] # Each element is indexed from zero just as usual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNUrUgcPuZh6"
      },
      "source": [
        "> A Tensor can be of almost any shape and size\n",
        "\n",
        "Typically, lowecase variable names are scalar or vectors while uppercase variables are Matrix or tensors\n",
        "### Random Tensors\n",
        "\n",
        "These are important because many NN start with random numbers and adjust those random numbers to better represent the data:\n",
        "\n",
        "`start with random numbers->look at data->update random numbers->look at data-> update random numbers->and so on till we've got a valuable result`\n",
        "\n",
        "Docs for [torch.rand()](https://pytorch.org/docs/stable/generated/torch.rand.html?highlight=rand#torch.rand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmQTmMuzvOkn",
        "outputId": "2313731f-a9c4-47b8-f87c-58bb5de442b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0.1386, 0.7440, 0.9878, 0.5108],\n",
              "         [0.5321, 0.8450, 0.2677, 0.6699],\n",
              "         [0.0888, 0.1059, 0.4332, 0.8291]]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a random tensor of size/shape (same in PyTorch)\n",
        "random_tensor = torch.rand(1,3,4) # There can be whatever number inside of this\n",
        "random_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOJuxxv7vXIP",
        "outputId": "776a4fe7-62c6-43b0-de12-719f145366a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_tensor.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4op6_Nrvxad",
        "outputId": "88ff010c-4755-4a13-ffe8-1ede15e42dad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([224, 224, 3]), 3)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a random tensor with similar shape to an image tensor\n",
        "random_image_size_tensor = torch.rand(size=(224,224,3)) # height, width, color channels, doesn't matter if color channels go first or last (the importance here is in data representation as numbers)\n",
        "random_image_size_tensor.shape, random_image_size_tensor.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rSPGPqYwSZB",
        "outputId": "b251dada-6287-41d9-bfba-39d11d8c6802"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 4, 2]),\n",
              " 3,\n",
              " tensor([[[0.4426, 0.0127],\n",
              "          [0.2551, 0.0558],\n",
              "          [0.1676, 0.1585],\n",
              "          [0.7066, 0.8128]],\n",
              " \n",
              "         [[0.9551, 0.1249],\n",
              "          [0.9956, 0.8754],\n",
              "          [0.4633, 0.3014],\n",
              "          [0.4179, 0.8220]],\n",
              " \n",
              "         [[0.5612, 0.1607],\n",
              "          [0.4615, 0.4341],\n",
              "          [0.0792, 0.1077],\n",
              "          [0.5190, 0.9709]],\n",
              " \n",
              "         [[0.1212, 0.6068],\n",
              "          [0.2011, 0.3566],\n",
              "          [0.8561, 0.0662],\n",
              "          [0.2991, 0.0978]]]))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Playing with torch.rand()\n",
        "random_2 = torch.rand(4,4,2) # torch.rand(size=(x,y,z)) is optional as size is the default parameter\n",
        "random_2.shape, random_2.ndim, random_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQIUOeiYwm8e"
      },
      "source": [
        "### Zeros and Ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p56ILI-Gwnx",
        "outputId": "c5895535-7063-44ec-b466-7012cb35d0a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a tensor of all zeros (useful to create masks, don't worry we'll get back to that in a second)\n",
        "zeros = torch.zeros(size=(3,4))\n",
        "zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7BsOUitG7Bl",
        "outputId": "5e0e9ccd-91a0-4f2b-8194-0b37ccea0f9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a tensor of all ones\n",
        "ones = torch.ones(size=(3,4)) # zeros is more common than ones, though usually random tensors are the most used\n",
        "ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgw5smbHHFOY",
        "outputId": "e6624ad4-abcb-448c-8a9f-ada4a76349cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.float32, torch.float32)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "zeros.dtype, ones.dtype # Check the data type of each tensor, defaults to torch.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHuB4iWWHLT2"
      },
      "source": [
        "### Creating a range of tensors and tensors-like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7GaKZGxHTU4",
        "outputId": "0cb5c469-f281-4a44-d065-3c98f36609d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-8a5363e3a320>:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  torch.range(1,10) # This one is deprecated and won't work properly in the future\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# use torch.range()\n",
        "torch.range(1,10) # This one is deprecated and won't work properly in the future"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvMTpOfpHWGx",
        "outputId": "3e262fea-6217-46a0-c0c8-36c7de956770"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Better use torch.arange()!!!\n",
        "one_to_ten = torch.arange(start=1, end=11, step=1)\n",
        "one_to_ten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkfaWg59HkSd",
        "outputId": "a319bcef-ac25-4ed5-93ef-4d6a8feabcb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating tensors like (it is equivalent to converting a given tensor to the set structure)\n",
        "ten_zeros = torch.zeros_like(input=one_to_ten)\n",
        "ten_zeros # This is a ten_zeros tensor like the one_to_ten tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOEGk2skH_-S"
      },
      "source": [
        "### Dealing with Tensor Data Types\n",
        "**Note:** Tensor datatypes is one of the 3 big errors you'll run into with PyTorch and deep learning:\n",
        "1. Tensors not right datatype\n",
        "2. Tensors not right shape\n",
        "3. Tensors not on the right device\n",
        "\n",
        "Take a look at [precision in computing](https://en.wikipedia.org/wiki/Precision_(computer_science))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQJYNGKQINkn",
        "outputId": "8392e5e1-903d-41c0-af74-fdfc37aea541"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([3., 6., 9.]), torch.float32)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# float 32 tensor\n",
        "float_32_tensor = torch.tensor([3.0, 6.0, 9.0], dtype=None) # Even though we specify the dtype as none, it will default to torch.float32\n",
        "float_32_tensor, float_32_tensor.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7LG7QI-Idgv",
        "outputId": "ddb033b8-de37-43e9-fbeb-6d986c934e94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([3., 6., 9.], dtype=torch.float16), torch.float16)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# float 16 tensor\n",
        "float_16_tensor = torch.tensor([3.0, 6.0, 9.0], dtype=torch.float16) # here the data type is set to float 16\n",
        "float_16_tensor, float_16_tensor.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIaZb0_mIoVz",
        "outputId": "2f02e850-fd1b-439a-b4af-2e8d550ba51c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([3., 6., 9.]), torch.float32)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# There are two more arguments that are important for the torch.tensor() method:\n",
        "# float 32 tensor\n",
        "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
        "                               dtype=None, # what data type is the tensor (e.g. float32 or float16 being the most widely used) :: has to do with the mathematical precision \\\n",
        "                               # Sacrifying precision diminishes computing complexity\n",
        "                               device=None, # This defaults to cpu, but \"cuda\" argument sends the tensor to the gpu. \\\n",
        "                               # Make sure that the tensors you are gonna operate with are stored on the same device\n",
        "                               requires_grad=False) # Wheter or not to track gradients with this tensors' operations\n",
        "float_32_tensor, float_32_tensor.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghd7sTBNI_V5",
        "outputId": "f9d9a728-8b14-47a4-e4ba-3d300b7d497b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([3., 6., 9.]),\n",
              " torch.float32,\n",
              " tensor([3., 6., 9.], dtype=torch.float16),\n",
              " torch.float16)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "float_16_tensor_2 = float_32_tensor.type(torch.float16)\n",
        "float_32_tensor, float_32_tensor.dtype, float_16_tensor_2, float_16_tensor_2.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyfKYmFeKgy3",
        "outputId": "80fe9e19-9fd5-4b43-8360-6f5bf56a201c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([ 9., 36., 81.]), torch.float32)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example of dtype mismatch during tensor operations\n",
        "test_tensor = float_16_tensor * float_32_tensor\n",
        "test_tensor, test_tensor.dtype # PyTorch solves some little problems that let the code run anyway, while during training you are more likely to get errors, so just be aware"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FntGHOC_K3sF",
        "outputId": "b069ee04-abda-40f0-f9bf-7edc3ce0bbc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([12., 30., 54.], dtype=torch.float16)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Even int and float datatypes might work!\n",
        "int_32_tensor = torch.tensor([4,5,6], dtype=torch.int32)\n",
        "int_32_tensor * float_16_tensor # Again it keeps the highest precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wZuFXKBM_zf"
      },
      "source": [
        "### Getting information from tensors (attributes)\n",
        "- Datatype - can use `tensor.dtype`\n",
        "- Shape - can use `tensor.shape` (note `tensor.size()` is a method)\n",
        "- Device - can use `tensor.device`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1xD4wBeNUMu",
        "outputId": "d4d8e788-7be0-46f8-975c-01f8517980e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.9704, 0.5595, 0.4738, 0.4833],\n",
              "        [0.5261, 0.0683, 0.9549, 0.4343],\n",
              "        [0.2052, 0.1141, 0.0089, 0.4282]])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a tensor\n",
        "some_tensor = torch.rand(3,4)\n",
        "some_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vtos5vF7NaBi",
        "outputId": "9586223b-9df2-4f83-de1b-11c6c06c4883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.9704, 0.5595, 0.4738, 0.4833],\n",
            "        [0.5261, 0.0683, 0.9549, 0.4343],\n",
            "        [0.2052, 0.1141, 0.0089, 0.4282]])\n",
            "Datatype of tensor: torch.float32\n",
            "Shape of tensor: torch.Size([3, 4])\n",
            "Device tensor is on: cpu\n"
          ]
        }
      ],
      "source": [
        "# Details of some_tensor\n",
        "print(some_tensor)\n",
        "print(f\"Datatype of tensor: {some_tensor.dtype}\")\n",
        "print(f\"Shape of tensor: {some_tensor.shape}\")\n",
        "print(f\"Device tensor is on: {some_tensor.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbScNXZLNyRH",
        "outputId": "3a2cc86d-f93a-4876-b1f7-e793c6634e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.3955, 0.8850, 0.4950],\n",
            "        [0.0245, 0.1421, 0.0597],\n",
            "        [0.0447, 0.0704, 0.0510],\n",
            "        [0.2230, 0.3806, 0.7403]])\n",
            "Datatype of tensor: torch.float32\n",
            "Shape of tensor: torch.Size([4, 3])\n",
            "Device tensor is on: cpu\n"
          ]
        }
      ],
      "source": [
        "# Some changes applied to some_tensor\n",
        "some_tensor = torch.rand(size=(3,4), device=\"cpu\") # Let's see if this works, if so, we could just as well change cpu to cuda / Yup, it worked\n",
        "\n",
        "# Is there a reshape in torch?\n",
        "some_tensor = some_tensor.reshape(shape=(4,3)) # Yes, there is\n",
        "\n",
        "# Details of some_tensor\n",
        "print(some_tensor)\n",
        "print(f\"Datatype of tensor: {some_tensor.dtype}\")\n",
        "print(f\"Shape of tensor: {some_tensor.shape}\")\n",
        "print(f\"Device tensor is on: {some_tensor.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F5dOIiDOC7v"
      },
      "source": [
        "### Manipulating Tensors\n",
        "NN are actually a lot of tensor operations done inside the NN architecture, so Tensor operations include:\n",
        "* Addition\n",
        "* Substraction\n",
        "* Multiplication (element-wise)\n",
        "* Division\n",
        "* Matrix multiplication (dot product)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRQ05xQ4PCBQ",
        "outputId": "40e2cbf4-76cd-4314-875c-62e8055995b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([11, 12, 13])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Addition\n",
        "# Create a tensor\n",
        "tensor = torch.tensor([1,2,3])\n",
        "tensor + 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiGaHrjyPG5q",
        "outputId": "ea2e7454-29c5-4292-b6a2-7fc88829ea2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([10, 20, 30])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Multiply tensor by 10\n",
        "tensor * 10 # As we don't reassign the tensor, original tensor keeps equal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW6wKp8iPMt4",
        "outputId": "36b30db9-9d3d-4dda-81b5-96d952b18cb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-9, -8, -7])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Substract 10\n",
        "tensor - 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShNzlAZLPQBv",
        "outputId": "632efde9-a06f-428b-b399-de7338b4e5eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([10, 20, 30])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Try out PyTorch built-in functions\n",
        "torch.mul(tensor, 10) # 'mul' is the element-wise operator from the torch library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kEFIogEPUP9",
        "outputId": "e3baaf2b-aa72-4093-afbe-99f273045a3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([11, 12, 13])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Same for addition\n",
        "torch.add(tensor, 10) # This are supposed to run faster on a GPU when tensor's device is set to cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhV6skh8PkFm",
        "outputId": "f2edd706-c298-4631-d51f-7804d0a70d0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-9, -8, -7])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# There must be one for substraction too!\n",
        "torch.subtract(tensor, 10) # There ya go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SLghI4RPqsH",
        "outputId": "72be1e04-d47a-48d5-b9cd-d928c4f11905"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(14)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# What about matrices multiplication?\n",
        "torch.matmul(tensor, tensor) # I guess this is gonna give me a dimensionality error as tensor is actually a vector, so it's trying to matmul two 1x3 matrices (inner Ds mismatch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgx_oU19P2Q0",
        "outputId": "e4bef3dc-62ac-4e69-d83f-5edd74494d95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(14)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensor @ tensor # wtf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3rAUNwyQi0H"
      },
      "source": [
        "Turns out this solves the problem by not actually performing the matrix multiplication on the original $A.*B$ dimensions. Instead, it calculates $A.*B^T$, so it makes sense we get an scalar as a result as $(1x3) .* (1x3)$ [incompatible 3x1 inner Dimensions] is thus converted to $(1x3) .* (3x1)$, resulting in 3x3 inner Ds an 1x1 outer Ds which is the same as a single value output. **Will only work with vectors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXmtPSc4TXMc"
      },
      "source": [
        "### Matrix multiplication\n",
        "\n",
        "Two ways to multiply matrices:\n",
        "1. Element-wise multiplication\n",
        "2. Matrix multiplication (probably the most used â€”and usefulâ€” operation)\n",
        "\n",
        "[More on dot product](https://www.mathsisfun.com/algebra/matrix-multiplying.html)\n",
        "\n",
        "The two rules of dot product:\n",
        "\n",
        "1. The **inner dimensions** must match:\n",
        "* `(3,2) @ (3,2)` this won't work\n",
        "* `(3,2) @ (2,3)` this will work\n",
        "2. The resulting matrix has the shape of the **outer dimensions**:\n",
        "* `(3,2) @ (2,3)` -> `(2,2)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUbqbKKrbp6j",
        "outputId": "82e3a165-f049-4e7f-ad6d-ef89c03d895c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.6025, 1.6583, 1.5889],\n",
              "        [2.6897, 2.3246, 2.3338],\n",
              "        [3.1338, 3.0967, 3.6109]])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Examples on matmul use\n",
        "torch.matmul(torch.rand(3,10), torch.rand(10,3)) # Results in (3,3) matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh2MIMrbWzjR",
        "outputId": "06a3df7c-54cf-4019-d929-5942107fe8e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3]) * tensor([1, 2, 3])\n",
            "Equals: tensor([1, 4, 9])\n"
          ]
        }
      ],
      "source": [
        "# Element wise multiplication\n",
        "print(tensor, \"*\", tensor)\n",
        "print(f\"Equals: {tensor * tensor}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjWb_S60XTSI",
        "outputId": "d0de1abd-e06c-43cd-958c-ea5c3536cfbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(14)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Matrix multiplication\n",
        "torch.matmul(tensor,tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO-aGdQxXbpI",
        "outputId": "860cee0b-edea-4cbd-c221-122933de70c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(14)\n",
            "CPU times: user 1.53 ms, sys: 0 ns, total: 1.53 ms\n",
            "Wall time: 1.53 ms\n"
          ]
        }
      ],
      "source": [
        "# Demonstration in execution time differences\n",
        "%%time\n",
        "value = 0\n",
        "for i in range(len(tensor)):\n",
        "  value += tensor[i] * tensor[i]\n",
        "print(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azCoXNqGX2wN",
        "outputId": "2becb6f4-5948-47ff-c60e-7d6731e163c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 97 Âµs, sys: 19 Âµs, total: 116 Âµs\n",
            "Wall time: 120 Âµs\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(14)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "torch.matmul(tensor,tensor) # There is a big difference in running time even though we are only using a cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBr4N8b9X8Ak"
      },
      "source": [
        "### One of the most common errors in machine learning is shape errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OmE_LxFb3Kj",
        "outputId": "b9f3a448-d33d-4ab8-c6d0-6ca2f03dda55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([3, 2]), torch.Size([3, 2]))"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NN has a lot of matrix multiplications, if there is a shape error it could halt the whole process\n",
        "tensor_A = torch.tensor([\n",
        "    [1,2],\n",
        "    [3,4],\n",
        "    [5,6]\n",
        "])\n",
        "\n",
        "tensor_B = torch.tensor([\n",
        "    [7,10],\n",
        "    [8, 11],\n",
        "    [9, 12]\n",
        "])\n",
        "\n",
        "# torch.mm(tensor_A, tensor_B) is the same as torch.matmul, its an alias\n",
        "tensor_A.shape, tensor_B.shape # They don't match"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ5MeulscKD4"
      },
      "source": [
        "To fix our shape issues we can manipulate the shape of one of our tensors using a **transpose**.\n",
        "\n",
        "A transpose switches the axes or dimensions of a given tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTu2WAPoceM6",
        "outputId": "eecd73f3-61fd-444a-c385-ecbdf93f6d9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 7, 10],\n",
              "        [ 8, 11],\n",
              "        [ 9, 12]])"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensor_B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57DROJdVc0-a",
        "outputId": "b23109c6-5a14-4b0c-f126-3311b14abf84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 7,  8,  9],\n",
              "        [10, 11, 12]])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensor_B.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0Ertlmkc5KP",
        "outputId": "2fb6efb3-b7ad-4db8-d999-648c99ed7107"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 27,  30,  33],\n",
              "        [ 61,  68,  75],\n",
              "        [ 95, 106, 117]])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now our shapes should match, soooo let's see\n",
        "torch.matmul(tensor_A, tensor_B.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9NQ88nkdLXN",
        "outputId": "34e3fa69-3a07-469f-f9f5-6b43e9dffcab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n",
            "New shapes: tensor_A = torch.Size([3, 2]) (same shape as above), tensor_B.T = torch.Size([2, 3])\n",
            "Multiplying: torch.Size([3, 2]) @ torch.Size([2, 3]) <- inner dimensions must match\n",
            "Output:\n",
            "\n",
            "tensor([[ 27,  30,  33],\n",
            "        [ 61,  68,  75],\n",
            "        [ 95, 106, 117]])\n",
            "\n",
            "Output shape: torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "# The matrix multiplication operation works when tensor_B is transposed\n",
        "print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\")\n",
        "print(f\"New shapes: tensor_A = {tensor_A.shape} (same shape as above), tensor_B.T = {tensor_B.T.shape}\")\n",
        "print(f\"Multiplying: {tensor_A.shape} @ {tensor_B.T.shape} <- inner dimensions must match\")\n",
        "print(\"Output:\\n\")\n",
        "output = torch.matmul(tensor_A, tensor_B.T)\n",
        "print(output)\n",
        "print(f\"\\nOutput shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF182mqrd5LZ"
      },
      "source": [
        "## Finding the min, max, mean, sum, etc (Tensor aggregation)\n",
        "\n",
        "Going from all elements to one element, or a few elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GGzYH-3wgvn",
        "outputId": "8b8094b1-02d1-47d5-b5df-c355c2137ded"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91])"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a tensor\n",
        "x = torch.arange(1,100,10)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNZHr3DOw8TF",
        "outputId": "ea17b940-d5bf-43d2-96a3-e4ce606ac80f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(1), tensor(1))"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the min\n",
        "torch.min(x), min(x) # Here the torch.min() will be better to use just like with most built in functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxPKAWfJxGVn",
        "outputId": "05edf86d-34a0-4b2b-91ad-bb380b0d8800"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(91), tensor(91))"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the max\n",
        "torch.max(x), x.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZEAl6g-wa_n",
        "outputId": "f64acc50-0a21-4876-de28-1da4e9ca7fb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(46.)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the mean\n",
        "torch.mean(x.type(dtype=torch.float32)) # This function works with float 32, so 'Long' won't work. Needs a cast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByhvpOHJwne_",
        "outputId": "516dfa04-17b2-44be-eb73-f6a079b7f1a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(460), tensor(460))"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the sum\n",
        "torch.sum(x), x.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdIAFcrgxQIh"
      },
      "source": [
        "## Finding the positional min and max (argmin, argmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17GvpA1dxWDt",
        "outputId": "dad9ba21-070f-484d-a14d-d4683bd5ac42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(1), tensor(0))"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the positional min ||find the position in tensor that has the minimum value and return its value\n",
        "x[x.argmin()], x.argmin() # This will be useful when using the softmax activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDhg_DScxZuc",
        "outputId": "99ce7c9a-4252-4395-ab36-9af44e5f853d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(91), tensor(9))"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find the positional max\n",
        "x[x.argmax()], x.argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OevQRANyFK1"
      },
      "source": [
        "## Reshaping, stacking, squeezing and unsqueezing tensors\n",
        "* Reshaping - reshapes an input tensor to a given shape\n",
        "* View - return a view of an input tensor of certain shape but keep the same memory as the original tensor\n",
        "* Stacking - combine multiple tensors on top of each other (`vstack`) or side by side (`hstack`)\n",
        "* Squeeze - removes all '1' dimensions from a tensor\n",
        "* Unsqueeze - add a '1' dimension to a target tensor\n",
        "* Permute - Return a view of the input with dimensions permuted (swapped) in a certain way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGC90TwUzEF9",
        "outputId": "e0a62469-b970-44e7-8c1a-eb0271914562"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.]),\n",
              " torch.Size([12]))"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a tensor\n",
        "x = torch.arange(1., 13.)\n",
        "x, x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6JucYmczN_1",
        "outputId": "93eb7766-65c2-4ce5-b573-871bfcc9f15f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[ 1.,  2.],\n",
              "          [ 3.,  4.]],\n",
              "\n",
              "         [[ 5.,  6.],\n",
              "          [ 7.,  8.]],\n",
              "\n",
              "         [[ 9., 10.],\n",
              "          [11., 12.]]]])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Add an extra dimension with tensor.reshape() [The new dimensions must be compatible with the original ones, multiply your dimensions to make sure they fit]\n",
        "x_reshaped = x.reshape(1,3,2,2) # x.shape = (12), 1*3*2*2 = 12\n",
        "x_reshaped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH9ZtegvzbJZ",
        "outputId": "e25f5ae1-2584-4a39-b304-ee8d9c4f75cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.],\n",
              "         [ 7.,  8.,  9.],\n",
              "         [10., 11., 12.]]),\n",
              " torch.Size([4, 3]))"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Change the view (Use the same memory space but handle the view from a different variable with a different dimensionality output)\n",
        "z = x.view(4,3)\n",
        "z, z.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xQQleOWjqc9",
        "outputId": "5c1b3bdd-30bb-4445-987c-b3bd3d8c0910"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 5.,  2.,  3.],\n",
              "         [ 5.,  5.,  6.],\n",
              "         [ 5.,  8.,  9.],\n",
              "         [ 5., 11., 12.]]),\n",
              " tensor([ 5.,  2.,  3.,  5.,  5.,  6.,  5.,  8.,  9.,  5., 11., 12.]))"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Changing z changes x (bc a view of a tensor shares the same memory as the original tensor)\n",
        "z[:, 0] = 5\n",
        "z, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcMOO3k2kGwr",
        "outputId": "2cf6f93a-dfa7-4e45-9c91-5528ce79b32c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 5.,  2.,  3.,  5.,  5.,  6.,  5.,  8.,  9.,  5., 11., 12.],\n",
              "        [ 5.,  2.,  3.,  5.,  5.,  6.,  5.,  8.,  9.,  5., 11., 12.],\n",
              "        [ 5.,  2.,  3.,  5.,  5.,  6.,  5.,  8.,  9.,  5., 11., 12.],\n",
              "        [ 5.,  2.,  3.,  5.,  5.,  6.,  5.,  8.,  9.,  5., 11., 12.]])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Stack tensors on top of each other (dim is a number between 0 and the number of dimensions of the input tensors)\n",
        "x_stacked = torch.stack([x,x,x,x], dim=0) # kind of like using vstack\n",
        "x_stacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJT03FCJkcJg",
        "outputId": "289ec69f-ef63-42e9-cc20-2277056e6355"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 5.,  5.,  5.,  5.],\n",
              "        [ 2.,  2.,  2.,  2.],\n",
              "        [ 3.,  3.,  3.,  3.],\n",
              "        [ 5.,  5.,  5.,  5.],\n",
              "        [ 5.,  5.,  5.,  5.],\n",
              "        [ 6.,  6.,  6.,  6.],\n",
              "        [ 5.,  5.,  5.,  5.],\n",
              "        [ 8.,  8.,  8.,  8.],\n",
              "        [ 9.,  9.,  9.,  9.],\n",
              "        [ 5.,  5.,  5.,  5.],\n",
              "        [11., 11., 11., 11.],\n",
              "        [12., 12., 12., 12.]])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Stack tensors on top of each other\n",
        "x_stacked = torch.stack([x,x,x,x], dim=1) # This is not equivalent to hstack though, as hstack would stack them as part of the same dimension\n",
        "x_stacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VyNJNs2krMg",
        "outputId": "8e384c7f-0cec-4716-d365-3981268d7258"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 5.,  2.,  3.,  5.,  5.,  6.,  5.,  8.,  9.,  5., 11., 12.],\n",
              "         [ 5.,  2.,  3.,  5.,  5.,  6.,  5.,  8.,  9.,  5., 11., 12.],\n",
              "         [ 5.,  2.,  3.,  5.,  5.,  6.,  5.,  8.,  9.,  5., 11., 12.],\n",
              "         [ 5.,  2.,  3.,  5.,  5.,  6.,  5.,  8.,  9.,  5., 11., 12.]]),\n",
              " tensor([ 5.,  2.,  3.,  5.,  5.,  6.,  5.,  8.,  9.,  5., 11., 12.,  5.,  2.,\n",
              "          3.,  5.,  5.,  6.,  5.,  8.,  9.,  5., 11., 12.,  5.,  2.,  3.,  5.,\n",
              "          5.,  6.,  5.,  8.,  9.,  5., 11., 12.,  5.,  2.,  3.,  5.,  5.,  6.,\n",
              "          5.,  8.,  9.,  5., 11., 12.]))"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Using vstack and hstack\n",
        "x_vs = torch.vstack([x,x,x,x])\n",
        "x_hs = torch.hstack([x,x,x,x])\n",
        "x_vs, x_hs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY9hp1XalifK",
        "outputId": "3a1a98eb-2578-4923-df95-fe45a8fb8725"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Previous tensor: tensor([[[[[ 1,  6],\n",
            "           [11, 16],\n",
            "           [21, 26],\n",
            "           [31, 36],\n",
            "           [41, 46],\n",
            "           [51, 56],\n",
            "           [61, 66],\n",
            "           [71, 76],\n",
            "           [81, 86],\n",
            "           [91, 96]]]]])\n",
            "Previous shape: torch.Size([1, 1, 1, 10, 2])\n",
            "\n",
            " New tensor: tensor([[ 1,  6],\n",
            "        [11, 16],\n",
            "        [21, 26],\n",
            "        [31, 36],\n",
            "        [41, 46],\n",
            "        [51, 56],\n",
            "        [61, 66],\n",
            "        [71, 76],\n",
            "        [81, 86],\n",
            "        [91, 96]])\n",
            "New shape: torch.Size([10, 2])\n"
          ]
        }
      ],
      "source": [
        "# Squeeze\n",
        "# Create a tensor with 'size 1' dimensions\n",
        "tensor_s = torch.arange(1,100,5)\n",
        "tensor_s = tensor_s.reshape(shape=(1,1,1,10,2)) # Has three 'size 1' dimensions\n",
        "\n",
        "# Remove all single dimensions from a target tensor\n",
        "print(f\"Previous tensor: {tensor_s}\")\n",
        "print(f\"Previous shape: {tensor_s.shape}\")\n",
        "\n",
        "# Remove extra dimensions from tensor_s\n",
        "print(f\"\\n New tensor: {tensor_s.squeeze()}\")\n",
        "print(f\"New shape: {tensor_s.squeeze().shape}\") # Has not any 'size 1' dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKiOUilWmHmX",
        "outputId": "5818377e-222e-4f44-9156-06d7275aac32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Previous target: tensor([[[[[ 1,  6],\n",
            "           [11, 16],\n",
            "           [21, 26],\n",
            "           [31, 36],\n",
            "           [41, 46],\n",
            "           [51, 56],\n",
            "           [61, 66],\n",
            "           [71, 76],\n",
            "           [81, 86],\n",
            "           [91, 96]]]]])\n",
            "Previous shape: torch.Size([1, 1, 1, 10, 2])\n",
            "\n",
            "New tensor: tensor([[[[[[ 1,  6],\n",
            "            [11, 16],\n",
            "            [21, 26],\n",
            "            [31, 36],\n",
            "            [41, 46],\n",
            "            [51, 56],\n",
            "            [61, 66],\n",
            "            [71, 76],\n",
            "            [81, 86],\n",
            "            [91, 96]]]]]])\n",
            "New shape: torch.Size([1, 1, 1, 1, 10, 2])\n"
          ]
        }
      ],
      "source": [
        "# What about unsqueeze? Adds a single dimension to a target tensor at a specific dim\n",
        "print(f\"Previous target: {tensor_s}\")\n",
        "print(f\"Previous shape: {tensor_s.shape}\")\n",
        "\n",
        "# Add an extra dimension with unsqueeze\n",
        "print(f\"\\nNew tensor: {tensor_s.unsqueeze(dim=0)}\")\n",
        "print(f\"New shape: {tensor_s.unsqueeze(dim=0).shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbl1IIzSnQyd",
        "outputId": "dad40d3c-e701-4493-e98a-c64705c8b9ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Previous shape: torch.Size([224, 224, 3])\n",
            "New shape: torch.Size([3, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "# torch.permute returns a 'view' with dimensions permuted (re arranged). Has a dims argument as the desired orden of dimensions\n",
        "x_original = torch.rand(size=(224,224,3)) # Image representation for height, width, color channels\n",
        "\n",
        "# Permute the original tensor to re arrange the axis (or dim) order\n",
        "x_permuted = x_original.permute(2, 0, 1) # shifts access 0->1, 1->2, and 2->0\n",
        "\n",
        "print(f\"Previous shape: {x_original.shape}\") # height, width, color channels\n",
        "print(f\"New shape: {x_permuted.shape}\") # Color channels, height, width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FARqzS9NqOgO",
        "outputId": "d6f5ae05-ffd3-4590-a25d-c47f3d9b649a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.7938)"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_original[-1, -1, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF7S6wInqafi",
        "outputId": "faeca928-ecb9-4095-fe53-f9b7829416cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5400)"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_original[-1, -1, -1] = 0.5400\n",
        "x_original[-1, -1, -1] # So PyTorch allows value assignations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5I77c45vqEF",
        "outputId": "93a0cd10-a7fb-420f-e5bd-46f57e2a5b85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5400)"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the value that was changes in x_original in x_permuted\n",
        "x_permuted[-1, -1, -1] # They are the same as it uses same memory, different view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcFrK85awHNF"
      },
      "source": [
        "## Indexing (selecting data from tensors)\n",
        "\n",
        "Indexing with PyTorch is similar to indexing with NumPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t6U4FQDwcZ4",
        "outputId": "60a27860-6996-4153-b69d-d168705a5079"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1, 2, 3],\n",
              "         [4, 5, 6],\n",
              "         [7, 8, 9]]])"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a tensor\n",
        "x = torch.arange(1,10).reshape(1,3,3)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wckan1FwjzM",
        "outputId": "23b07308-a4db-4d6a-ee06-90c98e82305c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6],\n",
              "        [7, 8, 9]])"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0] # Index on the first bracket dim=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETczfExLwpZm",
        "outputId": "9fc4a730-e852-4517-be62-69362bf9d48c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0, 0] # Index on the middle bracker dim=1. Same as x[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S4Q_uHzw0QN",
        "outputId": "87ddeb03-7741-402c-9386-8dbcc4e1f2f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,0,0] # Index on the inner dimension (dim=2) in this case the latest bracket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRqnbIc8xAwg",
        "outputId": "3c70c037-e521-4827-ed29-0ba576ed40df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3]])"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can also use ':' to select 'all' of a target dimension\n",
        "x[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijIoVXE9xuiQ",
        "outputId": "cef742b6-551e-4156-d7a8-1e40a6f71dcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2, 5, 8]])"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# e.g. get all value of 0th and 1st dimenions but only index 1 of the last dimension\n",
        "x[:,:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfcnqxgsyJ2O",
        "outputId": "f472ff22-ff5e-4a19-a3fa-78f6b5bef52d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get all values from 0th dimensions, but only the 1st value of dimensions 1 and 2\n",
        "x[:, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc0ckXLbzwlW",
        "outputId": "01260e67-7922-4fd6-b347-82137cd32279"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3]])"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0,:] # all values of 0th and 2nd dimension but only the 1st of the 1st dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NafjjERIz2_t",
        "outputId": "c4eb3764-db5a-44c8-c9b9-6af7517f485e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([9]), tensor([[3, 6, 9]]))"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Index on x to return 9, and (3,6,9)\n",
        "x[:,-1,-1], x[:,:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORtB89J91NxG"
      },
      "source": [
        "## PyTorch tensors & NumPy\n",
        "\n",
        "NumPy is a popular scientific Python numerical computing library, because of this PyTorch can interact with it\n",
        "* NumPy -> Tensor :: `torch.from_numpy(ndarray)`\n",
        "* Tensor -> NumPy :: `torch.Tensor.numpy()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icnUAY822ZVY",
        "outputId": "0c4e1f0c-0443-4cf7-ad60-5383afdbe4e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([1., 2., 3., 4., 5., 6., 7.]),\n",
              " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NumPy array to Tensor\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "array = np.arange(1.0, 8.0)\n",
        "tensor = torch.from_numpy(array) # warning: this could cause a dtype issue in case we get contrasting dtypes\n",
        "array, tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDtbVUDj4ErQ",
        "outputId": "5ef0038c-c15a-4271-dc88-9bc86570487b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(dtype('float64'), torch.float64)"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "array.dtype, tensor.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBG1SIME4xJM",
        "outputId": "456c8b2c-1116-4454-f2b7-8d30bfd5fe5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.arange(1.0, 8.0).dtype # Torch keeeps the dtype of the np array it receives, so if you want to keep float32 use torch.from_numpy(ndarray).type(torch.MyDtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csX-6KzG464c",
        "outputId": "08c18f29-40dd-40ca-8e8a-9bfb590525c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([1., 1., 1., 1., 1., 1., 1.]),\n",
              " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# If we use torch.from_numpy() we get a new memory space, so changing np array does not affect the tensor\n",
        "\n",
        "# Now lets get tensor->numpy\n",
        "tensor = torch.ones(7)\n",
        "numpy_tensor = tensor.numpy()\n",
        "tensor, numpy_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-5PP_Va6jPe",
        "outputId": "3727c767-713a-4072-d990-54a7f52964f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "numpy_tensor.dtype, tensor.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x34jpyX56n_2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
